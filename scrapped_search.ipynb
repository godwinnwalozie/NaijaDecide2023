{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mazi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mazi/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import re  # regex\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import collections\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")\n",
    "plt.style.use('default')\n",
    "import squarify\n",
    "\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dler = nltk.downloader.Downloader()\n",
    "# dler._update_index()\n",
    "# dler.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n"
     ]
    }
   ],
   "source": [
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../web_scrap_database/peterobi_data.csv\",usecols=[1,3,4])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Created</th>\n",
       "      <th>Source of Tweet</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-11 06:34:42+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>It's only a matter of time before this Buhari-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-09 14:11:52+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@stanbic got a replacement debit card not long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-08 16:58:13+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@PoliceNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-08 16:53:57+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@Chinomsy2 @blossommartins Seconded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-07 17:51:19+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@DavidHundeyin You did well my brother.  While...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date Created      Source of Tweet  \\\n",
       "0  2022-08-11 06:34:42+00:00  Twitter for Android   \n",
       "1  2022-08-09 14:11:52+00:00  Twitter for Android   \n",
       "2  2022-08-08 16:58:13+00:00  Twitter for Android   \n",
       "3  2022-08-08 16:53:57+00:00  Twitter for Android   \n",
       "4  2022-08-07 17:51:19+00:00  Twitter for Android   \n",
       "\n",
       "                                               Tweet  \n",
       "0  It's only a matter of time before this Buhari-...  \n",
       "1  @stanbic got a replacement debit card not long...  \n",
       "2                                          @PoliceNG  \n",
       "3                @Chinomsy2 @blossommartins Seconded  \n",
       "4  @DavidHundeyin You did well my brother.  While...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date Created       object\n",
       "Source of Tweet    object\n",
       "Tweet              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Created</th>\n",
       "      <th>Source of Tweet</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-11 06:34:42+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>It's only a matter of time before this Buhari-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-09 14:11:52+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@stanbic got a replacement debit card not long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-08 16:58:13+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@PoliceNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-08 16:53:57+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@Chinomsy2 @blossommartins Seconded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-07 17:51:19+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@DavidHundeyin You did well my brother.  While...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date Created      Source of Tweet  \\\n",
       "0 2022-08-11 06:34:42+00:00  Twitter for Android   \n",
       "1 2022-08-09 14:11:52+00:00  Twitter for Android   \n",
       "2 2022-08-08 16:58:13+00:00  Twitter for Android   \n",
       "3 2022-08-08 16:53:57+00:00  Twitter for Android   \n",
       "4 2022-08-07 17:51:19+00:00  Twitter for Android   \n",
       "\n",
       "                                               Tweet  \n",
       "0  It's only a matter of time before this Buhari-...  \n",
       "1  @stanbic got a replacement debit card not long...  \n",
       "2                                          @PoliceNG  \n",
       "3                @Chinomsy2 @blossommartins Seconded  \n",
       "4  @DavidHundeyin You did well my brother.  While...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date Created'] = pd.to_datetime(df['Date Created'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cleaning the tweets  tHIS ONE REMOVES or strips THE dupliate twitter hash tags e.g @ john @ timi, retains emorji , but makes space \\n\n",
    "# def remove_pattern(input_txt, pattern):\n",
    "#     r = re.findall(pattern, input_txt)\n",
    "#     for i in r:\n",
    "#         input_txt = re.sub(i, '', input_txt)        \n",
    "#     return input_txt\n",
    "# def clean_tweets(tweets):\n",
    "#     #remove twitter Return handles (RT @xxx:)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "#     #remove twitter handles (@xxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "#     #remove URL links (httpxxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    \n",
    "#     #remove special characters, numbers, punctuations (except for #)\n",
    "#     tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "#     #remove new line\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    \n",
    "#     return tweets\n",
    "\n",
    "\n",
    "# df['tweet_preprocessed'] = df.Tweet.apply(lambda x :clean_tweets(x) )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stopword remover and preprocessing  channges emorji to lol also strip multiple hash tags\n",
    "\n",
    "# stopwords = set(stopwords.words(\"english\"))\n",
    "# def clean_tweet(tweet):\n",
    "#     if type(tweet) == np.float:\n",
    "#         return \"\"\n",
    "#     temp = tweet.lower()\n",
    "#     temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "#     temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "#     temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "#     temp = re.sub(r'http\\S+', '', temp)\n",
    "#     temp = re.sub('[()!?]', ' ', temp)\n",
    "#     temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "#     temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "#     temp = temp.split()\n",
    "#     temp = [w for w in temp if not w in stopwords]\n",
    "#     temp = \" \".join(word for word in temp)\n",
    "#     return temp\n",
    "# df['tweet_preprocessed'] = df.Tweet.apply(lambda x :clean_tweet(x) )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removes only @ from hash tags and converts emoji to lol\n",
    "\n",
    "# def clean_lemmatize_token(tweet):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "#     cleaned = tweet.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "#     tokenized = word_tokenize(cleaned)\n",
    "#     filtered = [w for w in tokenized if not w in stop_words]\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized = []\n",
    "#     for word in filtered:\n",
    "#         lemmatized.append(lemmatizer.lemmatize(word))\n",
    "#     to_remove = ['rt','mention','sxsw','link',\"RT @[\\w]*:\"]\n",
    "#     lemmatized = [w for w in lemmatized if w not in to_remove]\n",
    "#     lemmatized = ' '.join(lemmatized)\n",
    "#     return lemmatized\n",
    "\n",
    "# df['tweet_preprocessed'] = df.Tweet.apply(lambda x :clean_lemmatize_token(x) )\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use with TFIDF \n",
    "\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# tok = WordPunctTokenizer()\n",
    "# # pat1 = r'@[A-Za-z0-9]+'\n",
    "# # pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "# # combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "# def tweet_cleaner(text):\n",
    "#     # soup = BeautifulSoup(text, 'lxml')\n",
    "#     # souped = soup.get_text()\n",
    "#     # stripped = re.sub(combined_pat, '', souped)\n",
    "#     # try:\n",
    "#     #     clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "#     # except:\n",
    "#     #     clean = stripped\n",
    "#     # letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "#     # lower_case = letters_only.lower()\n",
    "#     # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "#     # I will tokenize and join together to remove unneccessary white spaces\n",
    "#     words = tok.tokenize(lower_case)\n",
    "#     return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use with TFIDF \n",
    "# # Tokenization\n",
    "# tk = word_tokenize\n",
    "# df['Tweet'].apply(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use with TFIDF \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def cleaning_stopwords(text):\n",
    "#     return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "# df['tweet_stopword'] = df['Tweet'].apply(lambda x:cleaning_stopwords(x) )\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tweet_preprocessed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tweet_preprocessed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     text \u001b[39m=\u001b[39m [st\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m data]\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtweet_clean_lam\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtweet_preprocessed\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: stemming_on_text(x))\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtweet_clean_lam\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tweet_preprocessed'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df['tweet_clean_lam']= df['tweet_preprocessed'].apply(lambda x: stemming_on_text(x))\n",
    "df['tweet_clean_lam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>tweet_preprocessed</th>\n",
       "      <th>tweet_clean_lam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's only a matter of time before this Buhari-...</td>\n",
       "      <td>matter time buhariled administration begin bor...</td>\n",
       "      <td>matter time buhariled administration begin bor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@stanbic got a replacement debit card not long...</td>\n",
       "      <td>stanbic got replacement debit card long ago al...</td>\n",
       "      <td>stanbic got replacement debit card long ago al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@PoliceNG</td>\n",
       "      <td>policeng</td>\n",
       "      <td>policeng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Chinomsy2 @blossommartins Seconded</td>\n",
       "      <td>chinomsy2 blossommartins seconded</td>\n",
       "      <td>chinomsy2 blossommartins seconded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DavidHundeyin You did well my brother.  While...</td>\n",
       "      <td>davidhundeyin well brother interview ongoing d...</td>\n",
       "      <td>davidhundeyin well brother interview ongoing d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>@deshotcaller Please we are not part of those ...</td>\n",
       "      <td>deshotcaller please part group nigerian make u...</td>\n",
       "      <td>deshotcaller please part group nigerian make u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>@Gidi_Traffic @tholuinspire Na waoo...nwam bia...</td>\n",
       "      <td>giditraffic tholuinspire na waoonwam biara col...</td>\n",
       "      <td>giditraffic tholuinspire na waoonwam biara col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>@SalehMaitala Lol</td>\n",
       "      <td>salehmaitala lol</td>\n",
       "      <td>salehmaitala lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>@adebola_44 @uyasj @TheCitadelGCC @T_Bakare @p...</td>\n",
       "      <td>adebola44 uyasj thecitadelgcc tbakare penman smh</td>\n",
       "      <td>adebola44 uyasj thecitadelgcc tbakare penman smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>@LayemiOjo @JoanaNKolo Lol</td>\n",
       "      <td>layemiojo joanankolo lol</td>\n",
       "      <td>layemiojo joanankolo lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet  \\\n",
       "0   It's only a matter of time before this Buhari-...   \n",
       "1   @stanbic got a replacement debit card not long...   \n",
       "2                                           @PoliceNG   \n",
       "3                 @Chinomsy2 @blossommartins Seconded   \n",
       "4   @DavidHundeyin You did well my brother.  While...   \n",
       "..                                                ...   \n",
       "60  @deshotcaller Please we are not part of those ...   \n",
       "61  @Gidi_Traffic @tholuinspire Na waoo...nwam bia...   \n",
       "62                                  @SalehMaitala Lol   \n",
       "63  @adebola_44 @uyasj @TheCitadelGCC @T_Bakare @p...   \n",
       "64                         @LayemiOjo @JoanaNKolo Lol   \n",
       "\n",
       "                                   tweet_preprocessed  \\\n",
       "0   matter time buhariled administration begin bor...   \n",
       "1   stanbic got replacement debit card long ago al...   \n",
       "2                                            policeng   \n",
       "3                   chinomsy2 blossommartins seconded   \n",
       "4   davidhundeyin well brother interview ongoing d...   \n",
       "..                                                ...   \n",
       "60  deshotcaller please part group nigerian make u...   \n",
       "61  giditraffic tholuinspire na waoonwam biara col...   \n",
       "62                                   salehmaitala lol   \n",
       "63   adebola44 uyasj thecitadelgcc tbakare penman smh   \n",
       "64                           layemiojo joanankolo lol   \n",
       "\n",
       "                                      tweet_clean_lam  \n",
       "0   matter time buhariled administration begin bor...  \n",
       "1   stanbic got replacement debit card long ago al...  \n",
       "2                                            policeng  \n",
       "3                   chinomsy2 blossommartins seconded  \n",
       "4   davidhundeyin well brother interview ongoing d...  \n",
       "..                                                ...  \n",
       "60  deshotcaller please part group nigerian make u...  \n",
       "61  giditraffic tholuinspire na waoonwam biara col...  \n",
       "62                                   salehmaitala lol  \n",
       "63   adebola44 uyasj thecitadelgcc tbakare penman smh  \n",
       "64                           layemiojo joanankolo lol  \n",
       "\n",
       "[65 rows x 3 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,['Tweet','tweet_preprocessed','tweet_clean_lam']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER FOR SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tweet_clean_lam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tweet_clean_lam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentiments \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtweet_clean_lam\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: sentiments\u001b[39m.\u001b[39mpolarity_scores(x)[\u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mNegetive\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtweet_clean_lam\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: sentiments\u001b[39m.\u001b[39mpolarity_scores(x)[\u001b[39m'\u001b[39m\u001b[39mneu\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/B666AB0966AACA07/Users/godwi/Data_Science_ML/NaijaDecide2023/scrapped_search.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtweet_clean_lam\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: sentiments\u001b[39m.\u001b[39mpolarity_scores(x)[\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tweet_clean_lam'"
     ]
    }
   ],
   "source": [
    "sentiments = SentimentIntensityAnalyzer()\n",
    "df['Positive'] = df['tweet_clean_lam'].apply(lambda x: sentiments.polarity_scores(x)['pos'])\n",
    "df['Negetive'] = df['tweet_clean_lam'].apply(lambda x: sentiments.polarity_scores(x)['neu'])\n",
    "df['Neutral'] = df['tweet_clean_lam'].apply(lambda x: sentiments.polarity_scores(x)['neg'])\n",
    "df['compound'] = df['tweet_clean_lam'].apply(lambda x: sentiments.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = df.compound.values\n",
    "sentiment = []\n",
    "for i in score:\n",
    "    if i >=0.05 :\n",
    "        sentiment.append('positive')\n",
    "    elif i <= -0.05:\n",
    "        sentiment.append('negative')\n",
    "    else:\n",
    "        sentiment.append('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Date Created'),\n",
       " (1, 'Source of Tweet'),\n",
       " (2, 'Tweet'),\n",
       " (3, 'tweet_preprocessed'),\n",
       " (4, 'tweet_clean_lam'),\n",
       " (5, 'Positive'),\n",
       " (6, 'Negetive'),\n",
       " (7, 'Neutral'),\n",
       " (8, 'compound')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in enumerate( df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']= sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_preprocessed</th>\n",
       "      <th>tweet_clean_lam</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matter time buhariled administration begin bor...</td>\n",
       "      <td>matter time buhariled administration begin bor...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stanbic got replacement debit card long ago al...</td>\n",
       "      <td>stanbic got replacement debit card long ago al...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>policeng</td>\n",
       "      <td>policeng</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinomsy2 blossommartins seconded</td>\n",
       "      <td>chinomsy2 blossommartins seconded</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>davidhundeyin well brother interview ongoing d...</td>\n",
       "      <td>davidhundeyin well brother interview ongoing d...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>deshotcaller please part group nigerian make u...</td>\n",
       "      <td>deshotcaller please part group nigerian make u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>giditraffic tholuinspire na waoonwam biara col...</td>\n",
       "      <td>giditraffic tholuinspire na waoonwam biara col...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>salehmaitala lol</td>\n",
       "      <td>salehmaitala lol</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>adebola44 uyasj thecitadelgcc tbakare penman smh</td>\n",
       "      <td>adebola44 uyasj thecitadelgcc tbakare penman smh</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>layemiojo joanankolo lol</td>\n",
       "      <td>layemiojo joanankolo lol</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tweet_preprocessed  \\\n",
       "0   matter time buhariled administration begin bor...   \n",
       "1   stanbic got replacement debit card long ago al...   \n",
       "2                                            policeng   \n",
       "3                   chinomsy2 blossommartins seconded   \n",
       "4   davidhundeyin well brother interview ongoing d...   \n",
       "..                                                ...   \n",
       "60  deshotcaller please part group nigerian make u...   \n",
       "61  giditraffic tholuinspire na waoonwam biara col...   \n",
       "62                                   salehmaitala lol   \n",
       "63   adebola44 uyasj thecitadelgcc tbakare penman smh   \n",
       "64                           layemiojo joanankolo lol   \n",
       "\n",
       "                                      tweet_clean_lam sentiment  \n",
       "0   matter time buhariled administration begin bor...   neutral  \n",
       "1   stanbic got replacement debit card long ago al...  negative  \n",
       "2                                            policeng   neutral  \n",
       "3                   chinomsy2 blossommartins seconded   neutral  \n",
       "4   davidhundeyin well brother interview ongoing d...  negative  \n",
       "..                                                ...       ...  \n",
       "60  deshotcaller please part group nigerian make u...  positive  \n",
       "61  giditraffic tholuinspire na waoonwam biara col...   neutral  \n",
       "62                                   salehmaitala lol  positive  \n",
       "63   adebola44 uyasj thecitadelgcc tbakare penman smh  negative  \n",
       "64                           layemiojo joanankolo lol  positive  \n",
       "\n",
       "[65 rows x 3 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = sentiment\n",
    "tweet_final = df.loc[:,['tweet_preprocessed','tweet_clean_lam','sentiment']]\n",
    "tweet_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_vader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\godwi\\Data_Science_ML\\NaijaDecide2023\\election_sentiment.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/godwi/Data_Science_ML/NaijaDecide2023/election_sentiment.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tweet_vader\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mvalue_counts()\u001b[39m.\u001b[39mto_frame()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweet_vader' is not defined"
     ]
    }
   ],
   "source": [
    "tweet_vader.sentiment.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "tweet_vader.sentiment.value_counts().plot(kind = 'pie',autopct='%1.1f%%',explode=[0.03,0.03,0.03],textprops={'fontsize': 15})\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vader.sentiment.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (8,3))\n",
    "sns.barplot(data = tweet_vader.sentiment.value_counts().to_frame(), y = tweet_vader.sentiment.value_counts().to_frame().sentiment,\\\n",
    "    x =tweet_vader.sentiment.value_counts().to_frame().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vader['sentiment_encoded'] = tweet_vader.sentiment.apply(lambda x: 1 if x == 'positive' else -1 if x == 'negative' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_sentiment = tweet_vader\n",
    "Top_location = location_sentiment.pivot_table(index ='user_location', columns ='sentiment',\\\n",
    "    values ='sentiment_encoded',aggfunc='count').sort_values('positive',ascending=False)[:20]\n",
    "Top_location.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_source = tweet_vader['Source of Tweet'].value_counts().to_frame()[:5]\n",
    "tweet_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize =(15,7))\n",
    "sns.barplot(data =tweet_source, y = tweet_source['Source of Tweet'], x= tweet_source.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig = px.bar(tweet_source)\n",
    "# fig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "stopwords = STOPWORDS\n",
    "stopwords.update([\"https\", \"co\",\"I\",\"The\",\"s\"])\n",
    "\n",
    "text = \"\".join(tweet_vader.loc[:,['tweet_vader','sentiment']].tweet_vader)\n",
    "fig, ax = plt.subplots(figsize = (20,20))\n",
    "wc = WordCloud(stopwords=stopwords,max_words = 1500 , width = 1000 , height = 500,collocations=False).generate(text)\n",
    "plt.axis(\"off\")\n",
    "ax.imshow(wc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in text.split() if word not in stopwords]\n",
    "counted_words = collections.Counter(filtered_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(15):\n",
    "    words.append(letter)\n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular = pd.DataFrame(counted_words, index =[0]).T.reset_index().rename({\"index\":\"word\"},axis =1)\\\n",
    "    .rename({0 :\"count\"},axis =1).groupby(\"word\").sum().sort_values(\"count\",ascending= False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize = (20,6))\n",
    "sns.barplot(y = words, x = counts, palette= 'turbo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = []\n",
    "for i in tweet_vader.tweet:\n",
    "    if 'atiku' in i:\n",
    "        candidate.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peterobi_1 = df_cleaned[df_cleaned.tweet.str.contains('peterobi')]\n",
    "# peterobi_2 = df_cleaned[df_cleaned.tweet.str.contains('obi')]\n",
    "# peterobi_3 = df_cleaned[df_cleaned.tweet.str.contains('peter obi')]\n",
    "# peterobi_4 = df_cleaned[df_cleaned.tweet.str.contains('obidient')]\n",
    "\n",
    "# obi = pd.concat([peterobi_1,peterobi_2,peterobi_3,peterobi_4]).drop_duplicates()\n",
    "# obi.sentiment.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atiku_1 = df_cleaned[df_cleaned.tweet.str.contains('atiku')]\n",
    "atiku_2 = df_cleaned[df_cleaned.tweet.str.contains('atiku abubakar ')]\n",
    "atiku = pd.concat([atiku_1,atiku_2]).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_1 = df_cleaned[df_cleaned.tweet.str.contains('asiwajubolaahmed')]\n",
    "bat_2 = df_cleaned[df_cleaned.tweet.str.contains('tinubu')]\n",
    "bat_3 = df_cleaned[df_cleaned.tweet.str.contains('BAT')]\n",
    "bat_4 = df_cleaned[df_cleaned.tweet.str.contains('jagaban')]\n",
    "\n",
    "bat = pd.concat([peterobi_1,peterobi_2,peterobi_3,peterobi_4]).drop_duplicates()\n",
    "bat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inec_1 = df_cleaned[df_cleaned.tweet.str.contains('inec')]\n",
    "inec_2 = df_cleaned[df_cleaned.tweet.str.contains('inecnigeria')]\n",
    "\n",
    "inec = pd.concat([inec_1,inec_2]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspirants = pd.concat([obi,atiku,bat,inec]).drop_duplicates()\n",
    "aspirants[aspirants.tweet.str.contains('tinubu')].sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspirants[aspirants.tweet.str.contains('omok')].sentiment.value_counts().plot(kind ='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "686cb14c4d1b9c10293961ee2915b43d0c34036735ac88ced71396191e257b28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
